// services/gpt-chapter.js - Ollama Local LLM integration for intelligent chapter generation
const { Ollama } = require('ollama');
require('dotenv').config();

class GPTChapterService {
  constructor() {
    this.ollamaHost = process.env.OLLAMA_HOST || 'http://localhost:11434';
    this.modelName = process.env.OLLAMA_MODEL || 'yi:9b';

    try {
      this.client = new Ollama({ host: this.ollamaHost });
      console.log(`‚úÖ Ollama client initialized: ${this.ollamaHost}, model: ${this.modelName}`);
    } catch (error) {
      console.warn('‚ö†Ô∏è Ollama client initialization failed. Chapter generation will use fallback method.');
      console.error(error);
      this.client = null;
    }
  }

  // Generate chapters from transcript using Ollama
  async generateChapters(transcript, videoDuration) {
    if (!this.client) {
      console.log('‚ö†Ô∏è Using fallback chapter generation (no Ollama configured)');
      return this.fallbackChapterGeneration(transcript, videoDuration);
    }

    try {
      const prompt = this.buildChapterPrompt(transcript, videoDuration);
      
      console.log(`ü§ñ Requesting Ollama (${this.modelName}) to generate chapters...`);

      const response = await this.client.chat({
        model: this.modelName,
        messages: [
          {
            role: 'system',
            content: 'You are an expert video content analyzer. Your task is to analyze video transcripts and generate meaningful chapter divisions with titles and descriptions. You must respond in valid JSON format.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        options: {
          temperature: 0.7,
          num_predict: 4000
        },
        stream: false
      });

      const content = response.message.content;
      console.log('‚úÖ Ollama response received');

      // Parse JSON response
      const chapters = this.parseGPTResponse(content);
      
      return chapters;

    } catch (error) {
      console.error('‚ùå Ollama chapter generation error:', error);
      console.log('‚ö†Ô∏è Falling back to automatic chapter generation');
      return this.fallbackChapterGeneration(transcript, videoDuration);
    }
  }

  // Build prompt for Ollama
  buildChapterPrompt(transcript, videoDuration) {
    const { fullText, segments } = transcript;
    
    return `ËØ∑ÂàÜÊûê‰ª•‰∏ãËßÜÈ¢ëÂ≠óÂπïÂÜÖÂÆπÔºå‰∏∫Ëøô‰∏™Êó∂Èïø ${Math.round(videoDuration / 60)} ÂàÜÈíüÁöÑËßÜÈ¢ëÁîüÊàêÁ´†ËäÇÊó∂Èó¥ËΩ¥„ÄÇ

ËßÜÈ¢ëÂ≠óÂπïÂÜÖÂÆπÔºö
${fullText.substring(0, 8000)}${fullText.length > 8000 ? '...(ÂÜÖÂÆπÂ∑≤Êà™Êñ≠)' : ''}

Êó∂Èó¥Êà≥ÁâáÊÆµÔºàÂâç50‰∏™ÔºâÔºö
${segments.slice(0, 50).map(s => `[${this.formatTime(s.startTime)} - ${this.formatTime(s.endTime)}] ${s.text}`).join('\n')}

Ë¶ÅÊ±ÇÔºö
1. Ê†πÊçÆÂÜÖÂÆπÁöÑËØ≠‰πâÂèòÂåñÂíå‰∏ªÈ¢òËΩ¨Êç¢ÔºåÊô∫ËÉΩÂàíÂàÜÁ´†ËäÇ
2. ÊØè‰∏™Á´†ËäÇÂ∫îËØ•ÊúâÊ∏ÖÊô∞ÁöÑ‰∏ªÈ¢ò
3. Á´†ËäÇÈïøÂ∫¶ÈÄÇ‰∏≠ÔºåÂª∫ËÆÆÊØè‰∏™Á´†ËäÇ3-10ÂàÜÈíü
4. ‰∏∫ÊØè‰∏™Á´†ËäÇÁîüÊàêÊúâÊÑè‰πâÁöÑÊ†áÈ¢òÔºàÁÆÄÁü≠Á≤æÁÇºÔºå15Â≠ó‰ª•ÂÜÖÔºâ
5. ‰∏∫ÊØè‰∏™Á´†ËäÇÁîüÊàêÁÆÄË¶ÅÊèèËø∞Ôºà50Â≠ó‰ª•ÂÜÖÔºâ
6. Á´†ËäÇÁöÑËµ∑ÂßãÊó∂Èó¥ÂøÖÈ°ªÂØπÂ∫îÂÆûÈôÖÁöÑÂ≠óÂπïÊó∂Èó¥Êà≥

ËØ∑‰ª•JSONÊ†ºÂºèËøîÂõûÔºåÊ†ºÂºèÂ¶Ç‰∏ãÔºö
{
  "chapters": [
    {
      "index": 1,
      "startTime": 0,
      "endTime": 180.5,
      "title": "Á´†ËäÇÊ†áÈ¢ò",
      "description": "Á´†ËäÇÊèèËø∞",
      "keyPoints": ["Ë¶ÅÁÇπ1", "Ë¶ÅÁÇπ2"]
    }
  ]
}

Ê≥®ÊÑèÔºö
- Âè™ËøîÂõûJSONÔºå‰∏çË¶ÅÂåÖÂê´ÂÖ∂‰ªñËØ¥ÊòéÊñáÂ≠ó
- startTime Âíå endTime ÂøÖÈ°ªÊòØÊï∞Â≠óÔºàÁßíÔºâ
- Á°Æ‰øùÁ´†ËäÇÊó∂Èó¥ËøûÁª≠‰∏î‰∏çÈáçÂè†
- ÊúÄÂêé‰∏Ä‰∏™Á´†ËäÇÁöÑ endTime Â∫îËØ•Êé•ËøëËßÜÈ¢ëÊÄªÊó∂Èïø ${videoDuration} Áßí`;
  }

  // Parse LLM JSON response
  parseGPTResponse(content) {
    try {
      // Extract JSON from markdown code blocks if present
      let jsonStr = content;
      const jsonMatch = content.match(/```json\s*([\s\S]*?)\s*```/) || content.match(/```\s*([\s\S]*?)\s*```/);
      if (jsonMatch) {
        jsonStr = jsonMatch[1];
      }

      const data = JSON.parse(jsonStr);
      
      if (!data.chapters || !Array.isArray(data.chapters)) {
        throw new Error('Invalid response format: missing chapters array');
      }

      return data.chapters.map((ch, idx) => ({
        chapterIndex: ch.index || (idx + 1),
        startTime: parseFloat(ch.startTime),
        endTime: parseFloat(ch.endTime),
        title: ch.title || `Chapter ${idx + 1}`,
        description: ch.description || '',
        keyPoints: ch.keyPoints || []
      }));

    } catch (error) {
      console.error('‚ùå Failed to parse LLM response:', error);
      throw new Error(`Invalid LLM response format: ${error.message}`);
    }
  }

  // Fallback chapter generation (when LLM is not available)
  fallbackChapterGeneration(transcript, videoDuration) {
    const { segments } = transcript;
    const chapters = [];
    
    // Simple algorithm: create chapters every 5 minutes or based on silence gaps
    const chapterDuration = 300; // 5 minutes
    const numChapters = Math.max(1, Math.ceil(videoDuration / chapterDuration));
    
    for (let i = 0; i < numChapters; i++) {
      const startTime = i * chapterDuration;
      const endTime = Math.min((i + 1) * chapterDuration, videoDuration);
      
      // Find segments in this time range
      const chapterSegments = segments.filter(
        s => s.startTime >= startTime && s.startTime < endTime
      );
      
      // Generate title from first segment or generic
      const title = chapterSegments.length > 0 
        ? this.generateTitleFromText(chapterSegments[0].text)
        : `Á¨¨ ${i + 1} ÈÉ®ÂàÜ`;
      
      chapters.push({
        chapterIndex: i + 1,
        startTime: startTime,
        endTime: endTime,
        title: title,
        description: chapterSegments.slice(0, 3).map(s => s.text).join(' ').substring(0, 100),
        keyPoints: []
      });
    }
    
    return chapters;
  }

  // Generate simple title from text
  generateTitleFromText(text) {
    const words = text.trim().split(/\s+/).slice(0, 8).join(' ');
    return words.length > 20 ? words.substring(0, 20) + '...' : words;
  }

  // Format seconds to HH:MM:SS
  formatTime(seconds) {
    const h = Math.floor(seconds / 3600);
    const m = Math.floor((seconds % 3600) / 60);
    const s = Math.floor(seconds % 60);
    return `${h.toString().padStart(2, '0')}:${m.toString().padStart(2, '0')}:${s.toString().padStart(2, '0')}`;
  }
}

module.exports = GPTChapterService;
